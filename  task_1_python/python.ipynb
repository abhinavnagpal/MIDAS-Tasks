{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TASK 1: A Python Problem</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</h5><b>Instructions</b></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your codes are properly documented. We recommend the following:-\n",
    "Before each code block have a markdown block which mentions the following\n",
    "- What the code block is doing.\n",
    "- What is your intuition behind doing this? Why do you think it is useful?\n",
    "- For bigger code blocks also add comments in the block.\n",
    "- Keep everything in this notebook things which worked, things which did not work.\n",
    "- This notebook should be a snapshot of the process you follow to solve a Data\n",
    "  problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM\n",
    "##### You have to write a python script which can fetch all the tweets (as many as allowed by Twitter API) done by midas@IIITD twitter handle and dump the responses into JSONlines file.\n",
    "##### The other part of your script should be able to parse these JSONline files to display the following for every tweet in a tabular format.\n",
    "- The text of the tweet.\n",
    "- Date and time of the tweet.\n",
    "- The number of favorites/likes.\n",
    "- The number of retweets.\n",
    "- Number of Images present in Tweet. If no image returns None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "\n",
    "<p style=\"color:red\">Note: As it is not mentioned in the question to remove links or hashtags from the tweet text, I have not performed that task. However it can be easily remove by regular expressions or by tokenizing the tweet text and removing individually.</p>\n",
    "\n",
    "<p style=\"color:red\">Note: In part 2 of task one, I have stored the required data from tweet in an excel file</p>\n",
    "\n",
    "<p style=\"color:red\">Note: In part 1 of task one, tweet is appended one by one. Running the same block will again append the tweets at the end. That is why i have checked whether the file exists in the directory before appending.</p>\n",
    "\n",
    "<p style=\"color:red\">Note: In part 2 of task one, Date and time has been kept together. To separate them, we can use <br /> [date = datetime.datetime.strptime(string, \"%d %b %Y  %H:%M:%S.%f\")] <br />\n",
    "and access year,time etc. by date.year, date.time etc.</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries for completing the task\n",
    "\n",
    "import json\n",
    "import jsonlines as jsl\n",
    "import tweepy\n",
    "import datetime\n",
    "import xlsxwriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Step 1: Initialize access and consumer variables </H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = '1114284772665126912-yIWbS51Mh0Xri4Hs1keqRBLMmorQMX'\n",
    "ACCESS_SECRET = 'WpLwxRQuiATZT520FoJBR1YpesuBucasTJ4yOuywYJ4cL'\n",
    "CONSUMER_KEY = 'C6jm3wI4AttxBBLwPHeY3LfiE'\n",
    "CONSUMER_SECRET = 'xoFnaDnn4yib6IGNWZIFHCtt4P8ZKehaqQikbORZObtg0dvKAE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 2: Setup tweepy to access the API</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tweepy by authenticating the user credentials\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 3: Get twitter data from midasiiitd timeline and store them in a list</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = \"midasiiitd\"\n",
    "\n",
    "tweets_list = []\n",
    "\n",
    "# mode = extended because tweets with longer text are truncated  \n",
    "# get tweet one by one and append in list\n",
    "\n",
    "for status in tweepy.Cursor(api.user_timeline, id=USER,tweet_mode=\"extended\").items(): \n",
    "    tweets_list.append(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Step 4: Dump the tweets in a jsonl file </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Delete it first.\n"
     ]
    }
   ],
   "source": [
    "path = 'final.jsonl' # name of the file where tweets are stored\n",
    "\n",
    "# first check if file already present\n",
    "exists = os.path.isfile(path)\n",
    "if exists==False:\n",
    "\n",
    "    # To store them in jsonl format - tweet is appended one by one followed by a linebreak\n",
    "    for i in range(0,len(tweets_list)):\n",
    "        with open(path, 'a', encoding='utf8') as file: \n",
    "            json.dump(tweets_list[i]._json,file)\n",
    "            file.write('\\n')\n",
    "else:\n",
    "    print(\"File already exists. Delete it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Step 5: Function to get required data from a tweet </h4>\n",
    "\n",
    "<ul>\n",
    "    <li> Likes/favourites: Present in 'favourite_count'</li>\n",
    "    <li> Retweet count: Present in 'retweet_count'</li>\n",
    "    <li> Number of image: Check for 'media' in 'entities'. If found, check the file format of each item in 'media'</li>\n",
    "    <li> Date and time: Present in 'created_at'</li>\n",
    "    <li> Tweet text: Present in 'full_text'</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common image formats\n",
    "img_fmt = ['png','jpeg','jpg',\"gif\",\"bmp\"]\n",
    "\n",
    "# function to extract data from 'tweet'\n",
    "def get_data(tweet):\n",
    "    favourite_count = tweet['favorite_count']\n",
    "    retweet_count = tweet['retweet_count']\n",
    "    number_image=0\n",
    "    if ('media' not in tweet['entities']):\n",
    "        number_image = None\n",
    "    else:\n",
    "        for media in tweet['entities']['media']:\n",
    "            if media['media_url'][-3:] in img_fmt:\n",
    "                number_image+=1\n",
    "    if (\"retweeted_status\" in tweet):\n",
    "        text = tweet['retweeted_status']['full_text']\n",
    "    else:\n",
    "        text = tweet['full_text']\n",
    "    return [text,tweet['created_at'],favourite_count,retweet_count,number_image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Step 6: Reading tweet line by line from jsonl file and storing in an excel file</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinav/dev/newcvtest/lib/python3.6/site-packages/xlsxwriter/worksheet.py:909: UserWarning: Ignoring URL 'https://t.co/46MisEZT5n%20Deep%20learning%20for%20predicting%20aftershocks%20of%20large%20earthquakes.%20Besides%20offering%20better%20predictions,%20interpretations%20of%20the%20model%20suggest%20promising%20directions%20for%20new%20physical%20theories%20https://t.co/b4bAvo6YZG' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n",
      "/Users/abhinav/dev/newcvtest/lib/python3.6/site-packages/xlsxwriter/worksheet.py:909: UserWarning: Ignoring URL 'https://t.co/hYiWI7ntyk%20TensorFuzz%20automates%20the%20process%20of%20finding%20inputs%20that%20cause%20some%20specific%20testable%20behavior,%20like%20disagreement%20between%20float16%20and%20float32%20implementations%20of%20a%20neural%20network%20https://t.co/Nt9YX5kJXU' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n"
     ]
    }
   ],
   "source": [
    "# data to be obtain\n",
    "arr=[\"text\",\"date and time\",\"likes count\",\"retweet count\",\"image count\"]\n",
    "\n",
    "#create a workbook and sheet\n",
    "workbook = xlsxwriter.Workbook('final.xlsx') \n",
    "worksheet = workbook.add_worksheet() \n",
    "\n",
    "# add column headers from arr\n",
    "for i in range(0,len(arr)):\n",
    "    worksheet.write(0, i, arr[i]) \n",
    "\n",
    "# Each line in .jsonl file is a tweet\n",
    "# Use the function 'get_data' to extract info.\n",
    "\n",
    "row=1 \n",
    "for line in open(path):\n",
    "    tweet = json.loads(line)\n",
    "    l= get_data(tweet)\n",
    "    for i in range(0,len(l)):\n",
    "        worksheet.write(row, i, l[i]) \n",
    "    row+=1\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
